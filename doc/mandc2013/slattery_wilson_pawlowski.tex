%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  This is a sample LaTeX input file for your contribution to 
%  the MC2013 conference. Modified by R.C. Martineau at INL from A. 
%  Sood at LANL, from J. Wagner ORNL who obtained the original class 
%  file by Jim Warsa, LANL, 16 July 2002}
%
%  Please use it as a template for your full paper 
%    Accompanying/related file(s) include: 
%       1. Document class/format file: mc2013.cls
%       2. Sample Postscript Figure:   figure.eps
%       3. A PDF file showing the desired appearance: template.pdf 
%    Direct questions about these files to: richard.martinea@inl.gov
%
%    Notes: 
%      (1) You can use the "dvips" utility to convert .dvi 
%          files to PostScript.  Then, use either Acrobat 
%          Distiller or "ps2pdf" to convert to PDF format. 
%      (2) Different versions of LaTeX have been observed to 
%          shift the page down, causing improper margins.
%          If this occurs, adjust the "topmargin" value in the
%          mc2013.cls file to achieve the proper margins. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{mc2013}
%
%  various packages that you may wish to activate for usage 
\usepackage{graphicx}
\usepackage{tabls}
\usepackage{afterpage}
\usepackage{cites}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[mathcal]{euscript}

%\usepackage{epsf}
%
%
% Insert authors' names and short version of title in lines below
%
\newcommand{\authorHead}      % Author's names here
   {Slattery, Wilson, and Pawlowski}  
\newcommand{\shortTitle}      % Short title here
   {Data Transfer Kit}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   BEGIN DOCUMENT
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%
%      Headers and Footers
\afterpage{%
\fancyhf{}%
\fancyhead[CE]{              
{\scriptsize \authorHead}}                                                
\fancyhead[CO]{               
{\scriptsize \shortTitle}}                  
%\lfoot{\scriptsize{
%International Conference on Mathematics and Computational Methods
%Applied to Nuclear Science \& Engineering (M\&C 2013), 
%\\ Sun Valley, Idaho, USA, May 5-9, 2013.}}%
\rfoot{\thepage/\totalpages{}}%

\pagestyle{fancy}
%\setlength{\topmargin}{-20pt}
}
 
\normalsize

\setlength{\baselineskip}{16.8pt}
\vspace{-3pt}

% 
% TITLE
%

\begin{center}
\textbf{\large \\%
THE DATA TRANSFER KIT: A GEOMETRIC RENDEZVOUS-BASED TOOL FOR
MULTIPHYSICS DATA TRANSFER
}
% 
% FIRST AUTHORS 
%
\\
\setlength{\baselineskip}{14pt}
\textbf{S.R. Slattery and P.P.H. Wilson} \\
Department of Engineering Physics \\
University of Wisconsin - Madison \\
1500 Engineering Dr., Madison, WI 53706 \\
sslattery@wisc.edu; wilsonp@engr.wisc.edu \\

% 
% SECOND AUTHORS (if not needed delete from here) 
%
\vspace{12pt}
\textbf{R.P. Pawlowski}\\
Sandia National Laboratories \\
1515 Eubank SE, Albuquerque, NM 87123 \\
rppawlo@sandia.gov \\ 
%
% SECOND AUTHORS (to here)
%

\end{center}

%
% SET RAGGED RIGHT MARGIN
%
\raggedright


%%---------------------------------------------------------------------------%%
\section*{ABSTRACT} 
\begin{quote}
\begin{small}
The Data Transfer Kit (DTK) is a software component designed to
provide parallel data transfer services for arbitrary physics
components based on the concept of geometric rendezvous. The
rendezvous algorithm provides a means to geometrically correlate two
geometric domains that may be arbitrarily decomposed in a parallel
simulation. By repartitioning both domains such that they have the
same geometric domain on each parallel process, efficient and load
balanced search operations and data transfer can be performed at a
desirable algorithmic time complexity with low communication overhead
relative to other types of mapping algorithms. With the increased
development efforts in multiphysics simulation and other multiple mesh
and geometry problems, generating parallel topology maps for
transferring fields and other data between geometric domains is a
common operation. This document describes the algorithms used to
generate parallel topology maps based on the concept of geometric
rendezvous as implemented in DTK with examples using a conjugate heat
transfer calculation and thermal coupling with a surrogate neutronics
code. In addition, we provide the results of initial scaling studies
performed on the Jaguar Cray XK6 system at Oak Ridge National
Laboratory for a worse-case-scenario problem in terms of algorithmic
complexity that shows good scaling on $O(1 \times 10^4)$ cores for
topology map generation and good scaling on $O(1 \times 10^5)$ cores
for the data transfer operation with meshes of $O(1 \times 10^9)$
elements.


\emph{Key Words}: data transfer, multiphysics, rendezvous, parallel
computing
\end{small} 
\end{quote}

\setlength{\baselineskip}{14pt}
\normalsize

%%---------------------------------------------------------------------------%%
\Section{INTRODUCTION} 
\label{sec:intro}

In many physics applications, it is often desired to transfer fields
(i.e. degrees of freedom or other data) between geometric domains that
may or may not conform in physical space. In addition, for massively
parallel simulations, it is typical that geometric domains not only do
not conform spatially, but also that their parallel decompositions do
not correlate and are independent of one another due to physics-based
partitioning and discretization requirements. As an example, this
situation can occur in multiphysics simulations where physics fields
provide feedback between solution iterations \cite{Tautges_2009_2} or
parallel adaptive mesh simulations where fields must be moved between
meshes after refining and coarsening \cite{Edwards_2006}. It is
therefore desirable to have a set of tools to relate two geometric
domains of arbitrary parallel decomposition such that fields and other
data can be transferred between them.

The Data Transfer Kit (DTK) is a software component devloped as part
of the Consortium for Advanced Simulation of LWR's (CASL)
\cite{u.s._department_of_energy_casl_2011} designed to provide
parallel services for mesh and geometry searching and data transfer
based on the concept of geometric rendezvous as developed by Plimpton,
Hendrickson, and Stewart \cite{Plimpton_2004} originally implemented
as part of the SIERRA framework \cite{Stewart_2004}. Their work has
been extended to move towards a component design for use with
arbitrary physics codes such that varying representations of mesh,
geometry, and fields are able to access these services
\cite{Chand_2008}. In addition, the original mesh-based rendezvous
algorithms have been expanded to be used with both mesh and geometry
representations of the geometric domain. This document will briefly
outline the rendezvous algorithms as implemented in DTK. Examples of
data transfer using a conjugate heat transfer calculation and a
CFD-neutronics type coupling are provided. Parallel scaling results
are also presented for the mesh-based mapping algorithm for a
worse-case-scenario problem where communication costs are at a
maximum.


%%---------------------------------------------------------------------------%%
\Section{GEOMETRIC RENDEZVOUS} 
\label{sec:geometric_rendezvous}

Relating two non-conformal meshes will ultimately require some type of
evaluation algorithm to apply the data from one geometry to another
as. To drive these evaluation algorithms, the target objects to which
this data will be applied must be located within the the source
geometry. In a serial formulation, efficient search structures that
offer logarithmic asymptotic time complexity are available to perform
this operation. However, in a parallel formulation, if these two
geometries are arbitrarily decomposed, geometric alignment is not
likely and a certain degree of communication will be required. A
geometric rendezvous manipulates the source and target geometries such
that all geometric operations have a local formulation.

A geometry that is associated with the providing data through function
evaluations will be referred to as the source geometry while the
geometry that will be receiving the data will be referred to as the
target geometry. Although explicitly formulated with a source mesh and
target vertices below, these concepts can be applied to geometric
structures beyond mesh and vertices.

\Subsection{Mesh-Based Rendezvous Algorithm}
\label{subsec:rendezvous_algorithm}

The geometric rendezvous concept uses a global formulation for the
data transfer while maintaining a local formulation for the geometric
search operations by generating a secondary decomposition of the
geometric structures in the problem. In DTK, the algorithm developed by
Plimpton et. al. \cite{Plimpton_2004} for mesh-based data transfer
generates the rendezvous decomposition through global operations in
order to achieve a local framework for geometric operations.
\begin{enumerate}
\item Compute a box that bounds the source and target geometry
  intersection.
\item Create a rendezvous decomposition by performing recursive
  coordinate bisectioning on the source geometry \cite{Berger_1987}.
\item Send source geometry from the source decomposition to rendezvous
  decomposition.
\item Clone source geometry components which overlap into nearby
  recursive coordinate bisectioning sub-domains.
\item Build a kD-tree with the local mesh in each rendezvous
  partition \cite{Bentley_1975}.
\end{enumerate}
Using the above algorithm, a secondary decomposition of a subset of
the source mesh is generated forming the rendezvous decomposition. The
rendezvous decomposition is encapsulated as a separate entity from the
original geometric description of the domain. It can be viewed as a
copy of the source mesh subset that intersects the target
geometry. This copy has been repartitioned in a way that fundamental
geometric search operations are local and proceed globally in a load
balanced fashion.

With the rendezvous algorithm, we effectively have a search structure
that spans both parallel and physical space. We first search parallel
space by querying the rendezvous decomposition generated during
repartitioning. Global recursive coordinate bisectioning parameters
are maintained for global partitioning information, meaning that a
desintation process in the rendezvous decomposition can be determined
for any point on any process. Although this is a search over parallel
space, because of the geometric nature of the rendezvous decomposition
it is also a search over physical space with each process in the
rendezvous decomposition owning a specific subset of the mesh (with
marginal overlap at the boundaries).
 
Once points have been accumulated in the rendezvous decomposition, the
local kD-tree that is formed over the local mesh can be utilized. By
searching the kD-tree in logarithmic time, a subset of the mesh that
is in the vicinity of the target point is generated. This subset,
which is typically much smaller than the mesh owned by a particular
rendezvous process, is then searched with a more expensive
point-in-element operation that transforms the vertex into the
reference frame of each mesh element in the subset with a Newton
iteration strategy. This mapped point is then checked against the
canonical reference cell of that mesh element's topology to determine
if the vertex is contained within.

\Subsection{Parallel Topology Maps}

A set of mapping algorithms based on geometric rendezvous are
implemented within DTK that apply specifically to shared domain
problems. A shared domain problem is one in which the geometric
domains of the source and target intersect over all dimensions of the
problem. Figure~\ref{fig:shared_domain} gives an example of a shared
domain problem in 3 dimensions. Here, $\Omega(S)$ ({\sl yellow}) is
the source geometry, $\Omega(T)$ ({\sl blue}) is the target geometry,
and $\Omega(R)$ ({\sl red}) is their intersection and the shared
domain over which mapping and the rendezvous decomposition will be
generated.
\begin{figure}[htpb!]
  \centering \includegraphics[width=4in]{overlapping_domain.pdf}
  \caption{\bf \sl Shared domain example.} {\sl $\Omega(S)$ (yellow)
    is the source geometry, $\Omega(T)$ (blue) is the target geometry,
    and $\Omega(R)$ (red) is the shared domain.}
  \label{fig:shared_domain}
\end{figure}
The purpose of these mapping algorithms is to efficiently generate a
parallel topology map and the associated parallel communication plan
that can carry out the data transfer repeatedly with the minumum
required number of parallel messages and and data. A parallel topology
map is an operator, $M$, that defines the translation of a field,
$F(s): \mathbb{R}^D \rightarrow \mathbb{R}^N$, from a source spatial
and parallel domain, $\Omega_S$, to a field, $G(t): \mathbb{R}^D
\rightarrow \mathbb{R}^N$, in the target spatial and parallel domain
$\Omega_T$, such that $G(t)\leftarrow M(F(s))$ and $M: \mathbb{R}^D
\rightarrow \mathbb{R}^D, \forall r \in [\Omega_S \cap \Omega_T]$,
where $N$ is the dimensionality of the field and $D$ the
dimensionality of the spatial domain. It then follows that the
geometric rendezvous is defined as a geometric-based parallel
redistribution of the original source and target geometries defined
over the region $\Omega_R = \Omega_S \cap \Omega_T$.

\Subsection{Extension to General Geometries}
\label{subsec:general_geometry}

To handle software components that have a geometric entity-based
representation (e.g. a method of characteristics code with flat source
regions representing the geometry), the above algorithms have been
extended to operate on a general geometric description. The mesh-based
algorithm above is purely geometric, with each element in the mesh
treated as a separate geometric entity. If this is the case, then
other geometries such as annular rings, cubes, or any other irregular
shapes should apply to the algorithm. To extend the above algorithm,
we require a geometric entity to provide a small set of information
including point inclusion, the bounding box of the entity, centroid,
and dimensionality. With this information, the RCB parititoning can be
generated and the geometry repartitioned to the rendezvous
decomposition. With the RCB data we can then perform a proximity
search followed by the more expensive point inclusion checks. 

Given these new search structures, variations of the algorithms
presented in \cite{Plimpton_2004} can be extended beyond mesh-based
interpolation. DTK includes a geometric interpolation algorithm, very
similar to the original algorithm where data is applied to points in
the domain based on the field discretization in the geometry rather
than mesh elements. Geometry-to-geometry transfers are available when
the two physics components being coupled derive their geometric
description from the same source. This is useful in cases such as
transferring fuel temperatures from a geometric zone in one physics
application to the same geometric zone in another application in
parallel. Finally, for cases where volume-averaged quantities are
desired, typically an integral is performed. To support these
operations, DTK contains algorithms for volume averaging mesh-based
fields over the geometry that the mesh has discretized.

%%---------------------------------------------------------------------------%%
\Section{DATA TRANSFER EXAMPLES}
\label{sec:examples}

\Subsection{Conjugate Heat Transfer}
\label{subsec:cht}

\Subsection{CFD-Neutronics}
\label{subsec:cfd_neutronics}

%%---------------------------------------------------------------------------%%
\Section{PARALLEL SCALING STUDY}
\label{sec:scaling_study}

A typical use case of DTK is searching a mesh with a set of points and
applying field data to those points through function evaluations. For
this use case, a scaling study of the initial DTK implementation of
the rendezvous algorithm for data transfer was performed utilizing the
Jaguar Cray XK6 system at Oak Ridge National Laboratory. For each
study, a tri-linear hexahedron mesh was generated and decomposed
across the parallel domain. Each partition had one element in the z
direction while the x and y directions were varied to produce the
desired number of elements in the partition. All partitions in each
scaling study are square. This mesh is searched with random points
generated by sampling the x and y directions across the global mesh
domain.  By striding the random number seed used to generate the point
coordinates, each process will have a unique set of random points it
is searching for. For each scaling study, every process generated the
same number of random points as the number of elements on that
process, ensuring that a dense, all-to-all communication operation
will be required for mapping and data transfer. Once the points are
mapped to the mesh, the data transfer routine applies the process rank
in which they were found and transfers it back to the original owning
process for the point. In this way, because of the simple partitioning
used for the scaling studies, the results of the data transfer to the
random points can be independently verified by checking the applied
data against the expected mesh process rank.

A weak scaling study was performed by fixing the problem size per core
and increasing the number of cores used while a strong scaling study
was performed by fixing the total problem size while increasing the
number of cores used.

\Subsection{Weak Scaling}
\label{subsec:weak_scaling}
For the weak scaling study, the number of hexahedrons per partition
was fixed to 1.0E4 and the number of random points generated per
partition also fixed to 1.0E4. The number of cores used varied from 16
to 115,072. Figure~\ref{fig:weak_scaling} gives the results of the
weak scaling study. The largest case reported here uses 115,072 cores
and required 10.44 minutes for map generation and 0.48 seconds for
data transfer. It is clear from the weak scaling study that at high
numbers of processors communication latency begins to dominate for
this dense all-to-all problem as described above. However, it is worth
noting that once the mapping is complete, wall time for the actual
transfer of the data is several orders of magnitude less.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=5.5in]{WeakScaling.png}
  \caption{Weak scaling study results. The solid black curve reports
    the wall time to generate the mapping vs. number of processors
    while the solide red curve reports the wall time to transfer the
    data (apply) vs. number of processors. The dashed lines give
    perfect weak scaling the map generation (black) and the data
    transfer (red).}
  \label{fig:weak_scaling}
\end{figure}

Table~\ref{tab:weak_scaling}. Gives the raw data for the weak scaling
study. We note here that both the setup time (mapping) and the apply
time (data transfer) are relatively load balanced with average compute
times reported near the maximum compute time. In addition it should be
noted that for even the largest case at 115,072 cores the wall time is
not prohibitively large with the map generation routine requiring
approximately 10.44 minutes and the data transfer routine requiring
less than half a second.

The efficiency values for the map generation and data transfer
routines are also reported in table~\ref{tab:weak_efficiency} with the
16 core case used as the reference computation. In correlation with
the data presented in table~\ref{tab:weak_scaling}, the efficiencies
are observed to be low above 1,000 cores for this dense all-to-all
communication problem. For more physical coupling problems where the
communication is expected to be much sparser, this gives and idea of
just how sparse that communication must be for this algorithm to scale
well.

\begin{table}[htpb!]
  \begin{center}
    \begin{tabular}{llllllll}\hline\hline
      \multicolumn{1}{l}{} & 
      \multicolumn{1}{l}{Global} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Transfer} & 
      \multicolumn{1}{l}{Transfer} & 
      \multicolumn{1}{l}{Transfer}\\
      \multicolumn{1}{l}{Cores} & 
      \multicolumn{1}{l}{Elements} & 
      \multicolumn{1}{l}{Min (s)} & 
      \multicolumn{1}{l}{Max (s)} & 
      \multicolumn{1}{l}{Average (s)} & 
      \multicolumn{1}{l}{Min (s)} & 
      \multicolumn{1}{l}{Max (s)} & 
      \multicolumn{1}{l}{Average (s)}\\ \hline\hline
      %%
16 &	1.600E+05 & 2.63 &	2.65 &	  2.635 &	0.06 & 0.07 &	0.062 \\
128 &	1.280E+06 & 3.25 &	3.31 &	  3.30 &	0.06 &	0.07 &	0.063 \\
512 &	5.120E+06 & 3.58 &	3.86 &	  3.84 &	0.06 &	0.08 &	0.067 \\
1024 &	1.024E+07 & 3.98 &	4.53 &    4.48 &	0.06 &	0.09 &	0.076 \\
4096 &	4.096E+07 & 6.54 &	8.51 &	  8.39 &	0.13 &	0.15 &	0.141 \\
16384 &	1.638E+08 & 15.98 &	27.0 &	  23.73 &	0.28 &	0.32 &	0.296 \\
32768 &	3.277E+08 & 40.03 &	68.18 &   62.88 &	0.36 &	0.39 &	0.375 \\
65536 &	6.554E+08 & 214.69 & 239.21	& 234.76 &	0.41 &	0.45 & 0.429 \\
115072 & 1.151E+09 & 570.12 & 626.51	& 616.675 &	0.42 &	0.48 &	0.450 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\sl Weak scaling study data with the local problem size
    fixed to 1.0E4 elements/points. All times reported in
    seconds. Minimum, maximum, and average timing values are global
    and computed using the results from all processes.}
  \label{tab:weak_scaling}
\end{table}

\begin{table}[htpb!]
  \begin{center}
    \begin{tabular}{lll}\hline\hline
      \multicolumn{1}{c}{Cores}& 
      \multicolumn{1}{c}{Map Efficiency} & 
      \multicolumn{1}{c}{Transfer Efficiency}\\\hline\hline
      16 &	1.000 &	1.000 \\
      128 &	0.800 &	0.974 \\
      512 &	0.687 &	0.911 \\
      1024 &	0.581 &	0.812 \\
      4096 &	0.314 &	0.434 \\
      16384 &	0.111 &	0.206 \\
      32768 &	0.042 &	0.164 \\
      65536 &	0.011 &	0.143 \\
      115072 &	0.004 &	0.136 \\
      %%
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\sl Weak scaling efficiencies. The 16 process case was used
    as the reference case.}
  \label{tab:weak_efficiency}
\end{table}

Compared to the weak scaling results observed by Plimpton and
colleagues for a similar dense communication problem
\cite{Plimpton_2004}, these results show the same qualitative behavior
(see figure 7 in the reference). As the Jaguar system improves on all
aspects of machine performance over those used in the 2004 work, it is
expected that larger problems may be solved before the bandwidth
limiting behavior is observed.

\Subsection{Strong Scaling}
\label{subsec:strong_scaling}
For the strong scaling study, the global number of hexahedrons was
fixed to 1.0E8 and the global number of points fixed to 1.0E8 as well
with the number of cores varied from 256 to
65,536. Figure~\ref{fig:strong_scaling} gives the results of the
strong scaling study. Again, we note for the all-to-all communication
pattern required to map the random points that latency again begins to
dominate when a few thousand cores are used while the data transfer
scaling is excellent. The raw data for this study is presented in
table~\ref{tab:strong_scaling}. We see again that the algorithm is
relatively load balanced with average compute times near the maximum
reported compute times for the setup
operation. Table~\ref{tab:strong_efficiency} gives the efficiencies
computed for the strong scaling study with the 256 processor case used
as the reference.

\begin{figure}[htpb!]
  \centering
  \includegraphics[width=5.5in]{StrongScaling.png}
  \caption{Strong scaling study results. The solid black curve reports
    the wall time to generate the mapping vs. number of processors
    while the solide red curve reports the wall time to transfer the
    data (apply) vs. number of processors. The dashed lines give
    perfect strong scaling the map generation (black) and the data
    transfer (red).}
  \label{fig:strong_scaling}
\end{figure}

\begin{table}[htpb!]
  \begin{center}
    \begin{tabular}{llllllll}\hline\hline
      \multicolumn{1}{l}{}& 
      \multicolumn{1}{l}{Local} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Map} & 
      \multicolumn{1}{l}{Transfer} & 
      \multicolumn{1}{l}{Transfer} & 
      \multicolumn{1}{l}{Transfer}\\
      \multicolumn{1}{l}{Cores} & 
      \multicolumn{1}{l}{Elements} & 
      \multicolumn{1}{l}{Min (s)} & 
      \multicolumn{1}{l}{Max (s)} & 
      \multicolumn{1}{l}{Average (s)} & 
      \multicolumn{1}{l}{Min (s)} & 
      \multicolumn{1}{l}{Max (s)} & 
      \multicolumn{1}{l}{Average (s)}\\ \hline\hline
      %%
256 &	3.91E+05 & 149.58&	150.04 & 149.883 & 198.99 & 199.81 & 199.72 \\
1024 &	9.77E+04 & 36.08 &	36.68 &	36.60 &	5.52 &	5.59 &	5.55 \\
4096 &	2.44E+04 & 12.12 &	14.18 &	14.0558 &	0.37 &	0.45 &	0.04 \\
16384 &	6.10E+03 & 9.16 &	15.75 & 14.787	& 0.15 & 0.18 &	0.162 \\
32768 &	3.05E+03 & 7.42 &	17.92 &	14.33 & 0.06 &	0.10 &	0.080 \\
65536 &	1.53E+03 & 205.54 &	232.01 & 227.07 & 0.02 & 0.05 &	0.034 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\sl Strong scaling study data. All times reported in
    seconds. Minimum, maximum, and average timing values are global
    and computed using the results from all processes.}
  \label{tab:strong_scaling}
\end{table}

\begin{table}[htpb!]
  \begin{center}
    \begin{tabular}{lll}\hline\hline
      \multicolumn{1}{c}{Cores}& 
      \multicolumn{1}{c}{Map Efficiency} & 
      \multicolumn{1}{c}{Transfer Efficiency}\\\hline\hline
      %%
      256 &	1.000 &	1.000 \\
      1024 &	1.024 &	8.989 \\
      4096 &	0.666 &	27.84 \\
      16384 &	0.158 &	19.24 \\
      32768 &	0.082 &	19.62 \\
      65536 &	0.003 &	22.71 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\sl Strong scaling efficiencies. The 256 process case was used
    as the reference case.}
  \label{tab:strong_efficiency}
\end{table}

%%---------------------------------------------------------------------------%%
\Section{CONCLUSIONS}

We have presented the Data Transfer Kit, a new tool for parallel data
transfer for multiphysics applications. The concept of geometric
rendezvous is used to provide a collection of mesh and geometry-based
mappings for data transfer in shared domain problems. Initial scaling
studies have been completed for the Data Transfer Kit on the Jaguar
system. Their results show comparable qualitative behavior to the
literature results with improved performance due to the more advanced
computational resources available with good scaling for the data
transfer operation at $O(1 \times 10^5)$ cores. However, for the dense
communication patterns required to complete the scaling study problem,
poor weak scaling results are still observed above $O(1 \times 10^4)$
cores for the mapping operation. For data transfer problems where the
underlying mesh or geometry does not change, the wall times observed
for the mapping algorithm to be performed may not be prohibitive as
that operation will only be performed once during a setup phase for
the problem. Once the map is generated, it and the resulting parallel
communication plan can be used repeatedly in the data transfer
operation with excellent scaling and minimal wall time obeserved for
meshes of $O(1 \times 10^9)$ elements.

It is expected for more physical data transfer problems that the
overall communication pattern will be significantly sparser than the
problem presented here. Because of this, scaling for the mapping
algorithm is expected to improve for more physical problems. Further
scaling studies will be required to test this hypothesis. In addition,
the code has the potential to be extended to provide
surface-to-surface mappings for interface data transfers.


%%---------------------------------------------------------------------------%%
\section*{ACKNOWLEDGEMENTS}

This work was performed under funding provided by the Consortium for
Advanced Simulation of Light Water Reactors (CASL). The authors would
like to the the CASL Virtual Reactor Integration (VRI) development
team for their assistance over the course of development of DTK and
the Oak Ridge Leaderership Computing Facility (OLCF) for providing
computational resources and support.

%%---------------------------------------------------------------------------%%
%\Section*{REFERENCES}
\setlength{\baselineskip}{12pt}
\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}


